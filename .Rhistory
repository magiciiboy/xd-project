source('./Fig2Graph.R')
PAPERS_SPLITTED_BY_YEAR = T
CALCULATED_EDGES_PER_YEAR = T
CALCULATED_FRACTION = T
# Functions
splitPapersByYear <- function(bin=1) {
df_papers = read.csv('./data/GoogleScholar_paper_stats.csv')
years = c(1990:2015)
cols <- c("google_id", "year", "citations", "coauthor_codes")
# Five year bin
for (year in years) {
df_papers_year = filter(df_papers, X2014 <= year & X2014 > (year - bin))
colnames(df_papers_year) <- cols
write.csv(df_papers_year, paste0('./preprocessed/2b/papers_year_', year, '.csv'))
}
return(T)
}
processEdgesAndNodesPerYear <- function(year) {
df_papers <- read.csv(paste0('./preprocessed/2b/papers_year_', year, '.csv'))
df_nodes <- data.frame(matrix(ncol = 6, nrow = 0))
colnames(df_nodes) <- c('Id', 'Label', 'Interval', 'Weight', 'Dept', 'Orientation')
df_edges <- data.frame(matrix(ncol = 7, nrow = 0))
colnames(df_edges) <- c('Source', 'Target', 'Type', 'Id', 'Label', 'Interval', 'Weight')
# Read papers and create nodes and edges
for (row in 1:nrow(df_papers)) {
gid <- as.character(df_papers[row, "google_id"])
coauthor_codes <- as.character(df_papers[row, "coauthor_codes"])
coauthors <- unique(strsplit(coauthor_codes, ',')[[1]])
isXD = isCrossDisciplinaryByPollinators(coauthors)
orientation_xd = ifelse(isXD, 'XD', NA)
df_nodes <- addAuthorNode(df_nodes, gid, orientation = orientation_xd)
# Directed author
directed_coauthors = coauthors[coauthors != '0'
& coauthors != '1'
& coauthors != '2'
& coauthors != gid
& !is.na(coauthors)
& !is.null(coauthors)
& coauthors != '']
n_coauthors = length(directed_coauthors)
if(n_coauthors) {
for (row_dca in 1:n_coauthors) {
coauthor_gid <- directed_coauthors[row_dca]
# Add node and an edge to the falcuty
df_nodes <- addAuthorNode(df_nodes, coauthor_gid)
df_edges <- addEdge(df_edges, gid_source=gid, gid_target=coauthor_gid, add_mediate_links=T)
# Add edge between coauthors
if (n_coauthors > 1 && row_dca < n_coauthors) {
for (row_a_to_a in row_dca + 1:n_coauthors) {
next_coauthor_id <- directed_coauthors[row_a_to_a]
df_edges <- addEdge(df_edges, gid_source=coauthor_gid, gid_target=next_coauthor_id)
}
}
}
}
}
# Save
write.csv(df_nodes, paste0('./preprocessed/2b/nodes_year_', year, '.csv'))
write.csv(df_edges, paste0('./preprocessed/2b/edges_year_', year, '.csv'))
return(T)
}
aggreateEdgesForYears <- function() {
df_fraction <- data.frame(matrix(ncol = 6, nrow = 0))
colnames(df_fraction) <- c('xd_direct', 'xd_mediate', 'year')
xd_direct_count <- 0
xd_mediate_count <- 0
total_count <- 0
for (year in years) {
df_edges_year = read.csv(paste0('./preprocessed/2b/edges_year_', year, '.csv'))
xd_direct_count <- nrow(filter(df_edges_year, Type == 'Direct'))
xd_mediate_count <- nrow(filter(df_edges_year, Type == 'Mediate'))
xd_direct <- xd_direct_count / 2 * total_count
xd_mediate <- xd_mediate_count / 2 * total_count
df_fraction[nrow(df_fraction) + 1,] <- list(
xd_direct,
xd_mediate,
year
)
}
write.csv(FILE_NAME, df_fraction)
}
if (!PAPERS_SPLITTED_BY_YEAR) {
splitPapersByYear()
}
if (!CALCULATED_EDGES_PER_YEAR) {
years = c(1990:2015)
for(year in years) {
processEdgesAndNodesPerYear(year)
}
}
if (!CALCULATED_FRACTION) {
aggreateEdgesForYears()
}
# Author: Tung Huynh
library(tidyr)
FILE_NAME = "./preprocessed/collaborations_xd_fraction.csv"
library(dplyr)
source('./Fig2Config.R')
source('./Fig2Scholar.R')
source('./Fig2Graph.R')
PAPERS_SPLITTED_BY_YEAR = T
CALCULATED_EDGES_PER_YEAR = T
CALCULATED_FRACTION = T
# Functions
splitPapersByYear <- function(bin=1) {
df_papers = read.csv('./data/GoogleScholar_paper_stats.csv')
years = c(1990:2015)
cols <- c("google_id", "year", "citations", "coauthor_codes")
# Five year bin
for (year in years) {
df_papers_year = filter(df_papers, X2014 <= year & X2014 > (year - bin))
colnames(df_papers_year) <- cols
write.csv(df_papers_year, paste0('./preprocessed/2b/papers_year_', year, '.csv'))
}
return(T)
}
processEdgesAndNodesPerYear <- function(year) {
df_papers <- read.csv(paste0('./preprocessed/2b/papers_year_', year, '.csv'))
df_nodes <- data.frame(matrix(ncol = 6, nrow = 0))
colnames(df_nodes) <- c('Id', 'Label', 'Interval', 'Weight', 'Dept', 'Orientation')
df_edges <- data.frame(matrix(ncol = 7, nrow = 0))
colnames(df_edges) <- c('Source', 'Target', 'Type', 'Id', 'Label', 'Interval', 'Weight')
# Read papers and create nodes and edges
for (row in 1:nrow(df_papers)) {
gid <- as.character(df_papers[row, "google_id"])
coauthor_codes <- as.character(df_papers[row, "coauthor_codes"])
coauthors <- unique(strsplit(coauthor_codes, ',')[[1]])
isXD = isCrossDisciplinaryByPollinators(coauthors)
orientation_xd = ifelse(isXD, 'XD', NA)
df_nodes <- addAuthorNode(df_nodes, gid, orientation = orientation_xd)
# Directed author
directed_coauthors = coauthors[coauthors != '0'
& coauthors != '1'
& coauthors != '2'
& coauthors != gid
& !is.na(coauthors)
& !is.null(coauthors)
& coauthors != '']
n_coauthors = length(directed_coauthors)
if(n_coauthors) {
for (row_dca in 1:n_coauthors) {
coauthor_gid <- directed_coauthors[row_dca]
# Add node and an edge to the falcuty
df_nodes <- addAuthorNode(df_nodes, coauthor_gid)
df_edges <- addEdge(df_edges, gid_source=gid, gid_target=coauthor_gid, add_mediate_links=T)
# Add edge between coauthors
if (n_coauthors > 1 && row_dca < n_coauthors) {
for (row_a_to_a in row_dca + 1:n_coauthors) {
next_coauthor_id <- directed_coauthors[row_a_to_a]
df_edges <- addEdge(df_edges, gid_source=coauthor_gid, gid_target=next_coauthor_id)
}
}
}
}
}
# Save
write.csv(df_nodes, paste0('./preprocessed/2b/nodes_year_', year, '.csv'))
write.csv(df_edges, paste0('./preprocessed/2b/edges_year_', year, '.csv'))
return(T)
}
aggreateEdgesForYears <- function() {
df_fraction <- data.frame(matrix(ncol = 6, nrow = 0))
colnames(df_fraction) <- c('xd_direct', 'xd_mediate', 'year')
xd_direct_count <- 0
xd_mediate_count <- 0
total_count <- 0
for (year in years) {
df_edges_year = read.csv(paste0('./preprocessed/2b/edges_year_', year, '.csv'))
xd_direct_count <- nrow(filter(df_edges_year, Type == 'Direct'))
xd_mediate_count <- nrow(filter(df_edges_year, Type == 'Mediate'))
xd_direct <- xd_direct_count / 2 * total_count
xd_mediate <- xd_mediate_count / 2 * total_count
df_fraction[nrow(df_fraction) + 1,] <- list(
xd_direct,
xd_mediate,
year
)
}
write.csv(FILE_NAME, df_fraction)
}
if (!PAPERS_SPLITTED_BY_YEAR) {
splitPapersByYear()
}
if (!CALCULATED_EDGES_PER_YEAR) {
years = c(1990:2015)
for(year in years) {
processEdgesAndNodesPerYear(year)
}
}
if (!CALCULATED_FRACTION) {
aggreateEdgesForYears()
}
# Author: Tung Huynh
# This script generate output used in Gephi Application to
# create the network graph.
# There are 2 files to generate:
# - Nodes: (id, label, interval, size)
# - Edges: (source, target, type, id, label, interval, weight)
library(dplyr)
source('./Fig2Config.R')
source('./Fig2Scholar.R')
source('./Fig2Graph.R')
# Functions
splitPapersByYear <- function(bin=5) {
# This function splits the data by published year of
# a paper. Base on those preprocessed sub-dataset we
# will create nodes and edges for each year.
# According to the paper, the research orientation of
# each falcuty can shift from an original department
# to a cross-disciplinary at the year of cross-disciplinary
# paper has been published.
df_papers = read.csv('./data/GoogleScholar_paper_stats.csv')
years = c(1990, 1995, 2000, 2005, 2010, 2015)
cols <- c("google_id", "year", "citations", "coauthor_codes")
# Five year bin
for (year in years) {
df_papers_year = filter(df_papers, X2014 <= year & X2014 > (year - bin))
colnames(df_papers_year) <- cols
write.csv(df_papers_year, paste0('./preprocessed/papers_year_', year, '.csv'))
}
return(T)
}
processEdgesAndNodesPerYear <- function(year) {
df_papers <- read.csv(paste0('./preprocessed/papers_year_', year, '.csv'))
df_nodes <- data.frame(matrix(ncol = 6, nrow = 0))
colnames(df_nodes) <- c('Id', 'Label', 'Interval', 'Weight', 'Dept', 'Orientation')
df_edges <- data.frame(matrix(ncol = 7, nrow = 0))
colnames(df_edges) <- c('Source', 'Target', 'Type', 'Id', 'Label', 'Interval', 'Weight')
# Read papers and create nodes and edges
for (row in 1:nrow(df_papers)) {
gid <- as.character(df_papers[row, "google_id"])
coauthor_codes <- as.character(df_papers[row, "coauthor_codes"])
coauthors <- unique(strsplit(coauthor_codes, ',')[[1]])
isXD = isCrossDisciplinaryByPollinators(coauthors)
orientation_xd = ifelse(isXD, 'XD', NA)
df_nodes <- addAuthorNode(df_nodes, gid, orientation = orientation_xd)
# Directed author
directed_coauthors = coauthors[coauthors != '0'
& coauthors != '1'
& coauthors != '2'
& coauthors != gid
& !is.na(coauthors)
& !is.null(coauthors)
& coauthors != '']
n_coauthors = length(directed_coauthors)
if(n_coauthors) {
for (row_dca in 1:n_coauthors) {
coauthor_gid <- directed_coauthors[row_dca]
# Add node and an edge to the falcuty
df_nodes <- addAuthorNode(df_nodes, coauthor_gid)
df_edges <- addEdge(df_edges, gid_source=gid, gid_target=coauthor_gid)
# Add edge between coauthors
if (n_coauthors > 1 && row_dca < n_coauthors) {
for (row_a_to_a in row_dca + 1:n_coauthors) {
next_coauthor_id <- directed_coauthors[row_a_to_a]
df_edges <- addEdge(df_edges, gid_source=coauthor_gid, gid_target=next_coauthor_id)
}
}
}
}
}
# Elimiate nodes having no edges
df_nodes_cleaned <- data.frame(matrix(ncol = 6, nrow = 0))
colnames(df_nodes_cleaned) <- c('Id', 'Label', 'Interval', 'Weight', 'Dept', 'Orientation')
for (row in 1:nrow(df_nodes)) {
gid <- as.character(df_nodes[row, "Id"])
node_degree <- calculateNodeDegree(df_edges, gid)
if( node_degree > 0 ) {
xd <- (as.character(df_nodes[row, "Orientation"]) == 'XD')
node_coauthors <- getCoauthors(df_edges, gid)
node_orientation <- ifelse(xd, 'XD', getScholarOrientation(df_nodes, gid, node_coauthors))
# Add node
df_nodes_cleaned <- addAuthorNode(df_nodes_cleaned, gid, orientation=node_orientation, k=node_degree)
}
}
# Save
write.csv(df_nodes_cleaned, paste0('./preprocessed/nodes_year_', year, '.csv'))
write.csv(df_edges, paste0('./preprocessed/edges_year_', year, '.csv'))
return(T)
}
# Main
if (!DATA_SPLITTED_BY_YEARS) {
splitPapersByYear()
}
if (!DATA_PROCESSED_NODES_EDGES) {
# processEdgesAndNodesPerYear(1990)
# processEdgesAndNodesPerYear(1995)
# processEdgesAndNodesPerYear(2000)
# processEdgesAndNodesPerYear(2005)
processEdgesAndNodesPerYear(2010)
processEdgesAndNodesPerYear(2015)
}
setwd('~/Google Drive (tunghuynh314)/school/COCS 6323/COCS 6323 - Group Project/')
dfBIO <- read.csv('./data/Biology_citations_stats_CitationNormalizationData.csv')
dfCS <- read.csv('./data/ComputerScience_citations_stats_CitationNormalizationData.csv')
dfAll <- cbind(dfBIO, dfCS)
dfAll <- rbind(dfBIO, dfCS)
dim(dfBIO)
dim(dfCS)
dfCitations <- read.csv('./data/Faculty_GoogleScholar_Funding_Data_N4190.csv')
dim(dfCitations)
ead.csv('./data/Faculty_GoogleScholar_Funding_Data_N4190.csv')
dim(dfC)
dfC <- read.csv('./data/Faculty_GoogleScholar_Funding_Data_N4190.csv')
dim(dfC)
install.packages('sjstats')
# Model: FE
model_FE <- plm(z ~ ln_a + tau + I + factor(t), data=dat, index=c("i", "X"), model="within", effect="individual")
# install.packages('plm')
library(plm)
library(dplyr)
dat <- read.csv('./preprocessed/S4S5/panel_model_paper_citations_data_xd.csv')
print(colnames(dat))
print(nrow(dat))
plm(z ~ ln_a + tau + I + factor(t), data=dat, index=c("i", "X"), model="within", effect="individual")
summary(model_FE)
# Model: FE (Standardized)
# install.packages('sjstats')
library(sjstats)
model_FE_std <- std_beta(model_FE)
model_FE <- plm(z ~ ln_a + tau + I + factor(t), data=dat, index=c("i", "X"), model="within", effect="individual")
summary(model_FE)
# Model: FE (Standardized)
# install.packages('sjstats')
library(sjstats)
model_FE_std <- std_beta(model_FE)
# install.packages('plm')
library(plm)
library(dplyr)
dat <- read.csv('./preprocessed/S4S5/panel_model_paper_citations_data_all.csv')
print(colnames(dat))
print(nrow(dat))
# summary(dat)
# str(dat)
model_FE <- plm(z ~ ln_a + tau + I + factor(t), data=dat, index=c("i", "X"), model="within", effect="time")
summary(model_FE)
# install.packages('plm')
library(plm)
library(dplyr)
dat <- read.csv('./preprocessed/S4S5/panel_model_paper_citations_data_all.csv')
print(colnames(dat))
print(nrow(dat))
# summary(dat)
# str(dat)
model_FE <- plm(z ~ ln_a + tau + I + factor(t), data=dat, index=c("i", "X"), model="within", effect="time")
summary(model_FE)
# install.packages('plm')
library(plm)
library(dplyr)
dat <- read.csv('./preprocessed/S4S5/panel_model_paper_citations_data_all.csv')
print(colnames(dat))
print(nrow(dat))
# summary(dat)
# str(dat)
model_FE <- plm(z ~ ln_a + tau + I + factor(t), data=dat, index=c("i", "X"), model="within", effect="time")
summary(model_FE)
library(plm)
library(dplyr)
dat <- read.csv('./preprocessed/S4S5/panel_model_paper_citations_data_all.csv')
print(colnames(dat))
print(nrow(dat))
# summary(dat)
# str(dat)
model_FE <- plm(z ~ ln_a + tau + I + factor(t), data=dat, index=c("i", "X"), model="within", effect="time")
summary(model_FE)
# install.packages('plm')
library(plm)
library(dplyr)
dat <- read.csv('./preprocessed/S4S5/panel_model_paper_citations_data_all.csv')
print(colnames(dat))
print(nrow(dat))
# summary(dat)
# str(dat)
model_FE <- plm(z ~ ln_a + tau + I + factor(t), data=dat, index=c("i", "X"), model="within", effect="time")
summary(model_FE)
dat <- read.csv('./preprocessed/S4S5/panel_model_paper_citations_data_xd.csv')
head(dat)
help(log)
model_NoFE <- lm(z ~ ln_a + tau + I + factor(t) + log(PR) + log(lamda) + factor(dept), data=dat)
summary(model_NoFE)
# install.packages('plm')
library(plm)
library(dplyr)
dat <- read.csv('./preprocessed/S4S5/panel_model_paper_citations_data_xd.csv')
print(colnames(dat))
print(nrow(dat))
head(dat)
model_NoFE <- lm(z ~ ln_a + tau + I + factor(t) + log(PR) + log(lamda) + factor(dept), data=dat)
summary(model_NoFE)
model_NoFE_std <- lm(scale(z) ~ scale(ln_a) + scale(tau) + scale(I) + factor(t) + scale(log(PR)) + scale(log(lamda)) + factor(dept), data=dat)
summary(model_NoFE_std)
model_NoFE_std <- lm(scale(z) ~ scale(ln_a) + scale(tau) + scale(I) + factor(t) + scale(log(PR)) + scale(log(lamda)) + factor(dept), data=dat)
model_NoFE_std <- lm(scale(z) ~ scale(ln_a) + scale(tau) + scale(I) + factor(t) + log(PR) + log(lamda) + factor(dept), data=dat)
# install.packages('plm')
library(plm)
library(dplyr)
dat <- read.csv('./preprocessed/S4S5/panel_model_paper_citations_data_xd.csv')
print(colnames(dat))
print(nrow(dat))
head(dat)
model_NoFE <- lm(z ~ ln_a + tau + I + factor(t) + log(PR) + log(lamda) + factor(dept), data=dat)
summary(model_NoFE)
model_NoFE_std <- lm(scale(z) ~ scale(ln_a) + scale(tau) + scale(I) + factor(t) + log(PR) + log(lamda) + factor(dept), data=dat)
summary(model_NoFE_std)
# summary(dat)
# str(dat)
model_NoFE <- lm(z ~ ln_a + tau + I + factor(t) + PR + lamda + factor(dept), data=dat)
summary(model_NoFE)
# summary(dat)
# str(dat)
model_NoFE <- lm(z ~ ln_a + tau + I + factor(t) + PR + factor(dept), data=dat)
summary(model_NoFE)
+ lamda
# summary(dat)
# str(dat)
model_NoFE <- lm(z ~ ln_a + tau + I + factor(t) + PR + lamda + factor(dept), data=dat)
summary(model_NoFE)
model_NoFE_std <- lm(scale(z) ~ scale(ln_a) + scale(tau) + scale(I) + factor(t) + scale(PR) + scale(lamda) + factor(dept), data=dat)
summary(model_NoFE_std)
model_FE <- plm(z ~ ln_a + tau + I + factor(t), data=dat, index=c("i", "X"), model="within", effect="individual")
summary(model_FE)
# install.packages('plm')
library(plm)
library(plm)
library(dplyr)
dat <- read.csv('./preprocessed/S4S5/panel_model_paper_citations_data_xd.csv')
print(colnames(dat))
print(nrow(dat))
head(dat)
model_NoFE <- lm(z ~ ln_a + tau + I + factor(t) + PR + lamda + factor(dept), data=dat)
summary(model_NoFE)
# Remove row having NA (Ex: pagerank)
dat <- na.omit(dat)
colnames(dat)
nrow(dat)
model_FE <- plm(z ~ ln_a + tau + I + factor(t), data=dat, index=c("i", "X"), model="within", effect="individual")
summary(model_FE)
library(sjstats)
model_FE_std <- std_beta(model_FE)
model_FE_std
library(plm)
library(dplyr)
dat <- read.csv('./preprocessed/S4S5/panel_model_paper_citations_data_xd.csv')
print(colnames(dat))
print(nrow(dat))
head(dat)
# Remove row having NA (Ex: pagerank)
dat <- na.omit(dat)
print(nrow(dat))
# Model: FE (Standardized)
model_FE_std <- plm(scale(z) ~ scale(ln_a) + scale(tau) + scale(I) + factor(t), data=dat, index=c("i", "X"), model="within", effect="individual")
# Model: FE (Standardized)
model_FE_std <- plm(scale(z) ~ scale(ln_a) + scale(tau) + scale(I) + scale(factor(t)), data=dat, index=c("i", "X"), model="within", effect="individual")
# Model: FE (Standardized)
model_FE_std <- plm(scale(z) ~ scale(ln_a) + scale(tau) + scale(I), data=dat, index=c("i", "X"), model="within", effect="individual")
library(sjstats)
library(plm)
library(dplyr)
dat <- read.csv('./preprocessed/S4S5/panel_model_paper_citations_data_all.csv')
print(colnames(dat))
print(nrow(dat))
head(dat)
# Remove row having NA (Ex: pagerank)
dat <- na.omit(dat)
# summary(dat)
# str(dat)
# Model: No FE
model_NoFE <- lm(z ~ ln_a + tau + I + factor(t) + PR + lamda + factor(dept), data=dat)
summary(model_NoFE)
model_NoFE_std <- lm(scale(z) ~ scale(ln_a) + scale(tau) + scale(I) + factor(t) + scale(PR) + scale(lamda) + factor(dept), data=dat)
summary(model_NoFE_std)
dat <- read.csv('./preprocessed/S4S5/panel_model_paper_citations_data_all.csv')
print(nrow(dat))
head(dat)
# Remove row having NA (Ex: pagerank)
dat <- na.omit(dat)
print(nrow(dat))
# summary(dat)
# str(dat)
# Model: No FE
model_NoFE <- lm(z ~ ln_a + tau + I + factor(t) + PR + lamda + factor(dept), data=dat)
summary(model_NoFE)
jstats)
library(plm)
library(dplyr)
dat <- read.csv('./preprocessed/S4S5/panel_model_paper_citations_data_all.csv')
print(colnames(dat))
print(nrow(dat))
head(dat)
library(sjstats)
library(plm)
library(dplyr)
dat <- read.csv('./preprocessed/S4S5/panel_model_paper_citations_data_all.csv')
print(colnames(dat))
print(nrow(dat))
head(dat)
# Remove row having NA (Ex: pagerank)
dat <- na.omit(dat)
print(nrow(dat))
model_FE <- plm(z ~ ln_a + tau + I + factor(t), data=dat, index=c("i", "X"), model="within", effect="individual")
summary(model_FE)
sessionInfo()
sessionInfo()
library(plm)
library(dplyr)
dat <- read.csv('./preprocessed/S4S5/panel_model_paper_citations_data_all.csv')
print(colnames(dat))
print(nrow(dat))
head(dat)
# Remove row having NA (Ex: pagerank)
dat <- na.omit(dat)
print(nrow(dat))
