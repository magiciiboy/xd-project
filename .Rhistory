library(plm)
library(dplyr)
dat <- read.csv('./preprocessed/S4S5/panel_model_paper_citations_data_all.csv')
print(colnames(dat))
print(nrow(dat))
head(dat)
# Remove row having NA (Ex: pagerank)
dat <- na.omit(dat)
print(nrow(dat))
model_FE <- plm(z ~ ln_a + tau + I + factor(t), data=dat, index=c("i", "X"), model="within", effect="individual")
summary(model_FE)
sessionInfo()
sessionInfo()
library(plm)
library(dplyr)
dat <- read.csv('./preprocessed/S4S5/panel_model_paper_citations_data_all.csv')
print(colnames(dat))
print(nrow(dat))
head(dat)
# Remove row having NA (Ex: pagerank)
dat <- na.omit(dat)
print(nrow(dat))
# Model: No FE
model_NoFE <- lm(z ~ ln_a + tau + I + factor(t) + PR + lamda + factor(dept), data=dat)
summary(model_NoFE)
dat <- read.csv('./preprocessed/S4S5/panel_model_paper_citations_data_xd.csv')
# Model: No FE
model_NoFE <- lm(z ~ ln_a + tau + I + factor(t) + PR + lamda, data=dat)
summary(model_NoFE)
# install.packages('plm')
# install.packages('sjstats')
library(sjstats)
library(plm)
library(dplyr)
dat <- read.csv('./data/Faculty_GoogleScholar_Funding_Data_N4190.csv')
print(colnames(dat))
print(nrow(dat))
head(dat)
# Remove row having NA (Ex: pagerank). Only keep 3900 nodes in the network.
dat <- na.omit(dat)
print(nrow(dat))
# Remove row having NA (Ex: pagerank). Only keep 3900 nodes in the network.
dat <- dat %>% filter(PRCentrality > 0)
print(nrow(dat))
help(mutate)
# Preprocess the data
dat %>% mutate(C_i=t_pubs_citations)
dat %>% mutate(r_i=SchoolRank)
print(colnames(dat))
head(dat)
unique(dat$Y05yr)
summary(dat)
# Summarize the data characteristics
# head(dat)
# summary(dat)
str(dat)
# Treat TA values
colnames(dat)[colSums(is.na(dat)) > 0]
# Treat TA values
cols_NA <- colnames(dat)[colSums(is.na(dat)) > 0]
print(cols_NA)
# Treat TA values
is.na(dat$google_id)
# Treat TA values
colSums(is.na(dat$google_id))
# Treat TA values
rowSums(is.na(dat$google_id))
# Treat TA values
sum(is.na(dat$google_id))
help(sapply)
# Treat TA values
unlist(lapply(dat, function(x) any(is.na(x))))
unlist(lapply(df, function(x) any(x=0)))
unlist(lapply(df, function(x) any(x==0)))
unlist(lapply(df, function(x) any(x==0)))
unlist(lapply(dat, function(x) any(x==0)))
cols_0s <- unlist(lapply(dat, function(x) any(x==0)))
print(cols_0s[1])
print(cols_0s[1][1])
print(cols_0s[[1])
print(cols_0s[[1]])
print(cols_0s[[0]])
print(cols_0s[[2]])
print(cols_0s[[1]])
print(as.character(cols_0s[1]))
print(cols_0s[1])
cols_0s <- lapply(dat, function(x) any(x==0))
print(cols_0s[1])
help(unlist )
print(cols_0s$google_id)
cols_NA <- cols_NA[cols_NA==T]
cols_Os <- cols_Os[cols_Os==T]
cols_NA <- unlist(lapply(dat, function(x) any(is.na(x))))
cols_0s <- unlist(lapply(dat, function(x) any(x==0)))
cols_NA <- cols_NA[cols_NA==T]
cols_Os <- cols_Os[cols_Os==T]
cols_NA <- unlist(lapply(dat, function(x) any(is.na(x))))
cols_0s <- unlist(lapply(dat, function(x) any(x==0)))
cols_NA <- cols_NA[cols_NA==T]
cols_0s <- cols_0s[cols_0s==T]
print(cols_NA)
print(cols_0s)
# install.packages('plm')
# install.packages('sjstats')
library(sjstats)
library(plm)
library(dplyr)
dat <- read.csv('./data/Faculty_GoogleScholar_Funding_Data_N4190.csv')
print(colnames(dat))
print(nrow(dat))
# Summarize the data characteristics
# head(dat)
# summary(dat)
str(dat)
# Remove row having NA (Ex: pagerank). Only keep 3900 nodes in the network.
dat <- dat %>% filter(PRCentrality > 0)
# Preprocess the data
dat %>% mutate(C_i=t_pubs_citations)
dat %>% mutate(r_i=SchoolRank)
dat %>% mutate(dollar_NSF=t_deflated_nsf)
dat %>% mutate(dollar_NIH=t_deflated_nih)
dat %>% mutate(C_PR=PRCentrality)
dat %>% mutate(C_B=BetCentrality)
dat %>% mutate(chi=KMediated/KTotal)
# dat %>% mutate(C_D=Degree)
# Treat NA, zeros values
cols_NA <- unlist(lapply(dat, function(x) any(is.na(x))))
cols_0s <- unlist(lapply(dat, function(x) any(x==0)))
cols_NA <- cols_NA[cols_NA==T]
cols_0s <- cols_0s[cols_0s==T]
print(cols_NA)
print(cols_0s)
# Build models
print(nrow(dat))
model_CV <- lm()
dat <- read.csv('./data/Faculty_GoogleScholar_Funding_Data_N4190.csv')
dat <- dat %>% filter(PRCentrality > 0)
# Treat NA, zeros values
cols_NA <- unlist(lapply(dat, function(x) any(is.na(x))))
cols_0s <- unlist(lapply(dat, function(x) any(x==0)))
cols_NA <- cols_NA[cols_NA==T]
cols_0s <- cols_0s[cols_0s==T]
print(cols_NA)
print(cols_0s)
names(cols_0s)
print(names(cols_NA))
print(names(cols_0s))
head(dat)
# Build models
print(nrow(dat))
ZERO_REPLACEMENT_CONSTANT <- 0.00001
for (cname in names(cols_0s)) {
dat[cname] -> col
col[col==0] <- ZEROS_REPLACEMENT_CONSTANT
}
for (cname in names(cols_0s)) {
dat[cname] -> col
col[col==0] <- ZERO_REPLACEMENT_CONSTANT
}
unlist(lapply(dat, function(x) any(x==0)))
for (cname in names(cols_0s)) {
dat[cname] -> col
print(col)
col[col==0] <- ZERO_REPLACEMENT_CONSTANT
}
cols_NA <- unlist(lapply(dat, function(x) any(is.na(x))))
cols_0s <- unlist(lapply(dat, function(x) any(x==0)))
cols_NA <- cols_NA[cols_NA==T]
cols_0s <- cols_0s[cols_0s==T]
print(names(cols_NA))
print(names(cols_0s))
for (cname in names(cols_0s)) {
dat[cname] -> col
print(col[col==0])
# col[col==0] <- ZERO_REPLACEMENT_CONSTANT
}
for (cname in names(cols_0s)) {
dat[cname] -> col
print(col)
print(col[col==0])
# col[col==0] <- ZERO_REPLACEMENT_CONSTANT
}
ZERO_REPLACEMENT_CONSTANT <- 0.00001
dat <- read.csv('./data/Faculty_GoogleScholar_Funding_Data_N4190.csv')
print(colnames(dat))
print(nrow(dat))
# Summarize the data characteristics
# head(dat)
# summary(dat)
str(dat)
# Remove row having NA (Ex: pagerank). Only keep 3900 nodes in the network.
dat <- dat %>% filter(PRCentrality > 0)
# Treat NA, zeros values
cols_NA <- unlist(lapply(dat, function(x) any(is.na(x))))
cols_0s <- unlist(lapply(dat, function(x) any(x==0)))
cols_NA <- cols_NA[cols_NA==T]
cols_0s <- cols_0s[cols_0s==T]
print(names(cols_NA))
print(names(cols_0s))
for (cname in names(cols_0s)) {
dat[cname] -> col
print(cname)
print(col[col==0])
# col[col==0] <- ZERO_REPLACEMENT_CONSTANT
}
for (cname in names(cols_0s)) {
dat[cname] -> col
col[col==0] <- ZERO_REPLACEMENT_CONSTANT
dat[cname] <- col
}
unlist(lapply(dat, function(x) any(x==0)))
dat %>% mutate(C_i=t_pubs_citations)
dat$t_pubs_citations <- NULL
dat %>% mutate(r_i=SchoolRank)
dat$SchoolRank <- NULL
dat %>% mutate(dollar_NSF=t_deflated_nsf)
dat$t_deflated_nsf <- NULL
dat %>% mutate(dollar_NIH=t_deflated_nih)
dat$t_deflated_nih <- NULL
dat %>% mutate(C_PR=PRCentrality)
dat$PRCentrality <- NULL
dat %>% mutate(C_B=BetCentrality)
dat$BetCentrality <- NULL
# Build models
str(dat)
help("mutate")
colnames(dat)
# Preprocess the data
dat$C_i <- dat$t_pubs_citations
dat$t_pubs_citations <- NULL
colnames(dat)
dat$C_i
ZERO_REPLACEMENT_CONSTANT <- 0.00001
dat <- read.csv('./data/Faculty_GoogleScholar_Funding_Data_N4190.csv')
print(colnames(dat))
print(nrow(dat))
# Summarize the data characteristics
# head(dat)
# summary(dat)
str(dat)
# Remove row having NA (Ex: pagerank). Only keep 3900 nodes in the network.
dat <- dat %>% filter(PRCentrality > 0)
# Treat NA, zeros values
cols_NA <- unlist(lapply(dat, function(x) any(is.na(x))))
cols_0s <- unlist(lapply(dat, function(x) any(x==0)))
cols_NA <- cols_NA[cols_NA==T]
cols_0s <- cols_0s[cols_0s==T]
print(names(cols_NA))
print(names(cols_0s))
for (cname in names(cols_0s)) {
dat[cname] -> col
col[col==0] <- ZERO_REPLACEMENT_CONSTANT
dat[cname] <- col
}
colnames(dat)
# Preprocess the data
dat$C_i <- dat$t_pubs_citations
str(dat)
dat <- read.csv('./data/Faculty_GoogleScholar_Funding_Data_N4190.csv')
print(colnames(dat))
print(nrow(dat))
# Summarize the data characteristics
# head(dat)
# summary(dat)
str(dat)
# Remove row having NA (Ex: pagerank). Only keep 3900 nodes in the network.
dat <- dat %>% filter(PRCentrality > 0)
# Treat NA, zeros values
cols_NA <- unlist(lapply(dat, function(x) any(is.na(x))))
cols_0s <- unlist(lapply(dat, function(x) any(x==0)))
cols_NA <- cols_NA[cols_NA==T]
cols_0s <- cols_0s[cols_0s==T]
print(names(cols_NA))
print(names(cols_0s))
for (cname in names(cols_0s)) {
dat[cname] -> col
col[col==0] <- ZERO_REPLACEMENT_CONSTANT
dat[cname] <- col
}
# Preprocess the data
dat %>% mutate(C_i=t_pubs_citations)
dat %>% mutate(r_i=SchoolRank)
dat %>% mutate(dollar_NSF=t_deflated_nsf)
dat %>% mutate(dollar_NIH=t_deflated_nih)
dat %>% mutate(C_PR=PRCentrality)
dat %>% mutate(C_B=BetCentrality)
# dat %>% mutate(C_D=Degree)
# Remove old columns
dat$t_pubs_citations <- NULL
dat$SchoolRank <- NULL
dat$t_deflated_nsf <- NULL
dat$t_deflated_nih <- NULL
dat$PRCentrality <- NULL
dat$BetCentrality <- NULL
# Build models
str(dat)
print(nrow(dat))
ZERO_REPLACEMENT_CONSTANT <- 0.00001
dat <- read.csv('./data/Faculty_GoogleScholar_Funding_Data_N4190.csv')
print(colnames(dat))
print(nrow(dat))
# Summarize the data characteristics
# head(dat)
# summary(dat)
str(dat)
# Remove row having NA (Ex: pagerank). Only keep 3900 nodes in the network.
dat <- dat %>% filter(PRCentrality > 0)
# Treat NA, zeros values
cols_NA <- unlist(lapply(dat, function(x) any(is.na(x))))
cols_0s <- unlist(lapply(dat, function(x) any(x==0)))
cols_NA <- cols_NA[cols_NA==T]
cols_0s <- cols_0s[cols_0s==T]
print(names(cols_NA))
print(names(cols_0s))
for (cname in names(cols_0s)) {
dat[cname] -> col
col[col==0] <- ZERO_REPLACEMENT_CONSTANT
dat[cname] <- col
}
# Preprocess the data
dat %>% mutate(C_i=t_pubs_citations)
dat %>% mutate(r_i=SchoolRank)
dat %>% mutate(dollar_NSF=t_deflated_nsf)
dat %>% mutate(dollar_NIH=t_deflated_nih)
dat %>% mutate(C_PR=PRCentrality)
dat %>% mutate(C_B=BetCentrality)
# dat %>% mutate(C_D=Degree)
# Remove old columns
# dat$t_pubs_citations <- NULL
# dat$SchoolRank <- NULL
# dat$t_deflated_nsf <- NULL
# dat$t_deflated_nih <- NULL
# dat$PRCentrality <- NULL
# dat$BetCentrality <- NULL
# Build models
str(dat)
print(nrow(dat))
model_CV <- lm()
ZERO_REPLACEMENT_CONSTANT <- 0.00001
dat <- read.csv('./data/Faculty_GoogleScholar_Funding_Data_N4190.csv')
print(colnames(dat))
print(nrow(dat))
# Summarize the data characteristics
# head(dat)
# summary(dat)
str(dat)
# Remove row having NA (Ex: pagerank). Only keep 3900 nodes in the network.
dat <- dat %>% filter(PRCentrality > 0)
# Treat NA, zeros values
cols_NA <- unlist(lapply(dat, function(x) any(is.na(x))))
cols_0s <- unlist(lapply(dat, function(x) any(x==0)))
cols_NA <- cols_NA[cols_NA==T]
cols_0s <- cols_0s[cols_0s==T]
print(names(cols_NA))
print(names(cols_0s))
for (cname in names(cols_0s)) {
dat[cname] -> col
col[col==0] <- ZERO_REPLACEMENT_CONSTANT
dat[cname] <- col
}
# Preprocess the data
dat$C_i <- dat$t_pubs_citations
dat$r_i <- dat$SchoolRank
dat$dollar_NSF <- dat$t_deflated_nsf
dat$dollar_NIH <- dat$t_deflated_nih
dat$C_PR <- dat$PRCentrality
dat$C_B <- dat$BetCentrality
# dat %>% mutate(C_D=Degree)
# Remove old columns
# dat$t_pubs_citations <- NULL
# dat$SchoolRank <- NULL
# dat$t_deflated_nsf <- NULL
# dat$t_deflated_nih <- NULL
# dat$PRCentrality <- NULL
# dat$BetCentrality <- NULL
# Build models
str(dat)
print(nrow(dat))
model_CV <- lm(log(C_i) ~ log(h_index) + log(dollar_NSF) + log(num_nsf) + log(dollar_NIH) + log(num_nih), data=dat)
summary(model_CV)
# Model 1: CV
model_CV <- lm(log(C_i) ~ log(r_i) log(h_index) + log(dollar_NSF) + log(num_nsf) + log(dollar_NIH) + log(num_nih), data=dat)
# Model 1: CV
model_CV <- lm(log(C_i) ~ log(r_i) + log(h_index) + log(dollar_NSF) + log(num_nsf) + log(dollar_NIH) + log(num_nih), data=dat)
summary(model_CV)
dat$O <- dat$XDIndicator
model_CV <- lm(log(C_i) ~ log(r_i) + log(h_index) + log(dollar_NSF)
+ log(num_nsf) + log(dollar_NIH) + log(num_nih)
+ factor(O) + factor(Y05yr), data=dat)
summary(model_CV)
model_CV <- lm(log(C_i) ~ log(r_i) + log(h_index) + log(dollar_NSF)
+ log(num_nsf) + log(dollar_NIH) + log(num_nih), data=dat)
summary(model_CV)
model_CV <- lm(log(C_i) ~ log(r_i) + log(h_index) + log(dollar_NSF)
+ log(num_nsf) + log(dollar_NIH) + log(num_nih)
+ factor(O) + factor(Y05yr), data=dat)
summary(model_CV)
# install.packages('plm')
# install.packages('sjstats')
library(sjstats)
library(plm)
library(dplyr)
ZERO_REPLACEMENT_CONSTANT <- 0.00001
dat <- read.csv('./data/Faculty_GoogleScholar_Funding_Data_N4190.csv')
print(colnames(dat))
print(nrow(dat))
# Summarize the data characteristics
# head(dat)
# summary(dat)
str(dat)
# Treat NA, zeros values
cols_NA <- unlist(lapply(dat, function(x) any(is.na(x))))
cols_0s <- unlist(lapply(dat, function(x) any(x==0)))
cols_NA <- cols_NA[cols_NA==T]
cols_0s <- cols_0s[cols_0s==T]
print(names(cols_NA))
print(names(cols_0s))
for (cname in names(cols_0s)) {
dat[cname] -> col
col[col==0] <- ZERO_REPLACEMENT_CONSTANT
dat[cname] <- col
}
# Preprocess the data
dat$C_i <- dat$t_pubs_citations
dat$r_i <- dat$SchoolRank
dat$dollar_NSF <- dat$t_deflated_nsf
dat$dollar_NIH <- dat$t_deflated_nih
dat$C_PR <- dat$PRCentrality
dat$C_B <- dat$BetCentrality
dat$O <- dat$XDIndicator
# dat %>% mutate(C_D=Degree)
# Remove old columns
# dat$t_pubs_citations <- NULL
# dat$SchoolRank <- NULL
# dat$t_deflated_nsf <- NULL
# dat$t_deflated_nih <- NULL
# dat$PRCentrality <- NULL
# dat$BetCentrality <- NULL
# Build models
str(dat)
# Model 1: CV
print(nrow(dat))
model_CV <- lm(log(C_i) ~ log(r_i) + log(h_index) + log(dollar_NSF)
+ log(num_nsf) + log(dollar_NIH) + log(num_nih)
+ factor(O) + factor(Y05yr), data=dat)
summary(model_CV)
# Model 2: CV
# Remove row having NA (Ex: pagerank). Only keep 3900 nodes in the network.
datNet <- dat %>% filter(C_PR > 0)
print(nrow(datNet))
# install.packages('plm')
# install.packages('sjstats')
library(sjstats)
library(plm)
library(dplyr)
ZERO_REPLACEMENT_CONSTANT <- 0.00001
dat <- read.csv('./data/Faculty_GoogleScholar_Funding_Data_N4190.csv')
print(colnames(dat))
print(nrow(dat))
# Summarize the data characteristics
# head(dat)
# summary(dat)
# str(dat)
# Treat NA, zeros values
cols_NA <- unlist(lapply(dat, function(x) any(is.na(x))))
cols_0s <- unlist(lapply(dat, function(x) any(x==0)))
cols_NA <- cols_NA[cols_NA==T]
cols_0s <- cols_0s[cols_0s==T]
print(names(cols_NA))
print(names(cols_0s))
for (cname in names(cols_0s)) {
if (cname != 'PRCentrality') {
dat[cname] -> col
col[col==0] <- ZERO_REPLACEMENT_CONSTANT
dat[cname] <- col
}
}
# Preprocess the data
dat$C_i <- dat$t_pubs_citations
dat$r_i <- dat$SchoolRank
dat$dollar_NSF <- dat$t_deflated_nsf
dat$dollar_NIH <- dat$t_deflated_nih
dat$C_PR <- dat$PRCentrality
dat$C_B <- dat$BetCentrality
dat$O <- dat$XDIndicator
# dat %>% mutate(C_D=Degree)
# Remove old columns
# dat$t_pubs_citations <- NULL
# dat$SchoolRank <- NULL
# dat$t_deflated_nsf <- NULL
# dat$t_deflated_nih <- NULL
# dat$PRCentrality <- NULL
# dat$BetCentrality <- NULL
# Build models
# str(dat)
# Model 1: CV
print(nrow(dat))
model_CV <- lm(log(C_i) ~ log(r_i) + log(h_index) + log(dollar_NSF)
+ log(num_nsf) + log(dollar_NIH) + log(num_nih)
+ factor(O) + factor(Y05yr), data=dat)
summary(model_CV)
datNet <- dat %>% filter(C_PR > 0)
print(nrow(datNet))
model_CV_Net <- lm(log(C_i) ~ log(r_i) + log(h_index) + log(dollar_NSF)
+ log(num_nsf) + log(dollar_NIH) + log(num_nih)
+ log(C_PR) + Chi
+ factor(O) + factor(Y05yr), data=datNet)
summary(model_CV_Net)
std_beta(model_CV_Net)
# install.packages('plm')
# install.packages('sjstats')
library(sjstats)
